{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [ (\"big data\", 100, 15), (\"Hadoop\", 95, 25), (\"Python\", 75, 50),\n",
    "         (\"R\", 50, 40), (\"machine learning\", 80, 20), (\"statistics\", 20, 60),\n",
    "         (\"data science\", 60, 70), (\"analytics\", 90, 3),\n",
    "         (\"team player\", 85, 85), (\"dynamic\", 2, 90), (\"synergies\", 70, 0),\n",
    "         (\"actionable insights\", 40, 30), (\"think out of the box\", 45, 10),\n",
    "         (\"self-starter\", 30, 50), (\"customer focus\", 65, 15),\n",
    "         (\"thought leadership\", 35, 35)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_unicode(text: str) -> str:\n",
    "    return text.replace(u\"\\u2019\", \"'\")\n",
    "\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.oreilly.com/ideas/what-is-data-science\"\n",
    "html = requests.get(url).text\n",
    "soup = BeautifulSoup(html, 'html5lib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = soup.find(\"div\", \"main-post-radar-content\")   # find article-body div\n",
    "regex = r\"[\\w']+|[\\.]\"                       # matches a word or a period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = []\n",
    "\n",
    "for paragraph in content(\"p\"):\n",
    "    words = re.findall(regex, fix_unicode(paragraph.text))\n",
    "    document.extend(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "transitions = defaultdict(list)\n",
    "for prev, current in zip(document, document[1:]):\n",
    "    transitions[prev].append(current)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_using_bigrams() -> str:\n",
    "    current = \".\"   # this means the next word will start a sentence\n",
    "    result = []\n",
    "    while True:\n",
    "        next_word_candidates = transitions[current]    # bigrams (current, _)\n",
    "        current = random.choice(next_word_candidates)  # choose one at random\n",
    "        result.append(current)                         # append it to results\n",
    "        if current == \".\": return \" \".join(result)     # if \".\" we're done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Machine Learning course is very flexible schema in computer science â but in the data as it to mathematics to many of data product or product reviews the companies using the unique signature in Philadelphia county sheriffâ s quote that nobody remembers says it .'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_using_bigrams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_transitions = defaultdict(list)\n",
    "starts = []\n",
    "\n",
    "for prev, current, next in zip(document, document[1:], document[2:]):\n",
    "\n",
    "    if prev == \".\":              # if the previous \"word\" was a period\n",
    "        starts.append(current)   # then this is a start word\n",
    "\n",
    "    trigram_transitions[(prev, current)].append(next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_using_trigrams() -> str:\n",
    "    current = random.choice(starts)   # choose a random starting word\n",
    "    prev = \".\"                        # and precede it with a '.'\n",
    "    result = [current]\n",
    "    while True:\n",
    "        next_word_candidates = trigram_transitions[(prev, current)]\n",
    "        next_word = random.choice(next_word_candidates)\n",
    "\n",
    "        prev, current = current, next_word\n",
    "        result.append(current)\n",
    "\n",
    "        if current == \".\":\n",
    "            return \" \".join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No one in the publishing industry .'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_using_trigrams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import List, Dict\n",
    "\n",
    "# Type alias to refer to grammars later\n",
    "Grammar = Dict[str, List[str]]\n",
    "\n",
    "grammar = {\n",
    "    \"_S\"  : [\"_NP _VP\"],\n",
    "    \"_NP\" : [\"_N\",\n",
    "             \"_A _NP _P _A _N\"],\n",
    "    \"_VP\" : [\"_V\",\n",
    "             \"_V _NP\"],\n",
    "    \"_N\"  : [\"data science\", \"Python\", \"regression\"],\n",
    "    \"_A\"  : [\"big\", \"linear\", \"logistic\"],\n",
    "    \"_P\"  : [\"about\", \"near\"],\n",
    "    \"_V\"  : [\"learns\", \"trains\", \"tests\", \"is\"]\n",
    "}\n",
    "\n",
    "def is_terminal(token: str) -> bool:\n",
    "    return token[0] != \"_\"\n",
    "\n",
    "def expand(grammar: Grammar, tokens: List[str]) -> List[str]:\n",
    "    for i, token in enumerate(tokens):\n",
    "        # If this is a terminal token, skip it.\n",
    "        if is_terminal(token): continue\n",
    "\n",
    "        # Otherwise, it's a non-terminal token,\n",
    "        # so we need to choose a replacement at random.\n",
    "        replacement = random.choice(grammar[token])\n",
    "\n",
    "        if is_terminal(replacement):\n",
    "            tokens[i] = replacement\n",
    "        else:\n",
    "            # Replacement could be e.g. \"_NP _VP\", so we need to\n",
    "            # split it on spaces and splice it in.\n",
    "            tokens = tokens[:i] + replacement.split() + tokens[(i+1):]\n",
    "\n",
    "        # Now call expand on the new list of tokens.\n",
    "        return expand(grammar, tokens)\n",
    "\n",
    "    # If we get here we had all terminals and are done\n",
    "    return tokens\n",
    "\n",
    "def generate_sentence(grammar: Grammar) -> List[str]:\n",
    "    return expand(grammar, [\"_S\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst = generate_sentence(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['big', 'Python', 'near', 'big', 'Python', 'tests', 'data science']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import random\n",
    "\n",
    "def roll_a_die() -> int:\n",
    "    return random.choice([1, 2, 3, 4, 5, 6])\n",
    "\n",
    "def direct_sample() -> Tuple[int, int]:\n",
    "    d1 = roll_a_die()\n",
    "    d2 = roll_a_die()\n",
    "    return d1, d1 + d2\n",
    "\n",
    "def random_y_given_x(x: int) -> int:\n",
    "    \"\"\"equally likely to be x + 1, x + 2, ... , x + 6\"\"\"\n",
    "    return x + roll_a_die()\n",
    "\n",
    "def random_x_given_y(y: int) -> int:\n",
    "    if y <= 7:\n",
    "        # if the total is 7 or less, the first die is equally likely to be\n",
    "        # 1, 2, ..., (total - 1)\n",
    "        return random.randrange(1, y)\n",
    "    else:\n",
    "        # if the total is 7 or more, the first die is equally likely to be\n",
    "        # (total - 6), (total - 5), ..., 6\n",
    "        return random.randrange(y - 6, 7)\n",
    "\n",
    "def gibbs_sample(num_iters: int = 100) -> Tuple[int, int]:\n",
    "    x, y = 1, 2 # doesn't really matter\n",
    "    for _ in range(num_iters):\n",
    "        x = random_x_given_y(y)\n",
    "        y = random_y_given_x(x)\n",
    "    return x, y\n",
    "\n",
    "def compare_distributions(num_samples: int = 1000) -> Dict[int, List[int]]:\n",
    "    counts = defaultdict(lambda: [0, 0])\n",
    "    for _ in range(num_samples):\n",
    "        counts[gibbs_sample()][0] += 1\n",
    "        counts[direct_sample()][1] += 1\n",
    "    return counts\n",
    "\n",
    "def sample_from(weights: List[float]) -> int:\n",
    "    \"\"\"returns i with probability weights[i] / sum(weights)\"\"\"\n",
    "    total = sum(weights)\n",
    "    rnd = total * random.random()      # uniform between 0 and total\n",
    "    for i, w in enumerate(weights):\n",
    "        rnd -= w                       # return the smallest i such that\n",
    "        if rnd <= 0: return i          # weights[0] + ... + weights[i] >= rnd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    [\"Hadoop\", \"Big Data\", \"HBase\", \"Java\", \"Spark\", \"Storm\", \"Cassandra\"],\n",
    "    [\"NoSQL\", \"MongoDB\", \"Cassandra\", \"HBase\", \"Postgres\"],\n",
    "    [\"Python\", \"scikit-learn\", \"scipy\", \"numpy\", \"statsmodels\", \"pandas\"],\n",
    "    [\"R\", \"Python\", \"statistics\", \"regression\", \"probability\"],\n",
    "    [\"machine learning\", \"regression\", \"decision trees\", \"libsvm\"],\n",
    "    [\"Python\", \"R\", \"Java\", \"C++\", \"Haskell\", \"programming languages\"],\n",
    "    [\"statistics\", \"probability\", \"mathematics\", \"theory\"],\n",
    "    [\"machine learning\", \"scikit-learn\", \"Mahout\", \"neural networks\"],\n",
    "    [\"neural networks\", \"deep learning\", \"Big Data\", \"artificial intelligence\"],\n",
    "    [\"Hadoop\", \"Java\", \"MapReduce\", \"Big Data\"],\n",
    "    [\"statistics\", \"R\", \"statsmodels\"],\n",
    "    [\"C++\", \"deep learning\", \"artificial intelligence\", \"probability\"],\n",
    "    [\"pandas\", \"R\", \"Python\"],\n",
    "    [\"databases\", \"HBase\", \"Postgres\", \"MySQL\", \"MongoDB\"],\n",
    "    [\"libsvm\", \"regression\", \"support vector machines\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "K = 4\n",
    "\n",
    "# a list of Counters, one for each document\n",
    "document_topic_counts = [Counter() for _ in documents]\n",
    "\n",
    "# a list of Counters, one for each topic\n",
    "topic_word_counts = [Counter() for _ in range(K)]\n",
    "\n",
    "# a list of numbers, one for each topic\n",
    "topic_counts = [0 for _ in range(K)]\n",
    "\n",
    "# a list of numbers, one for each document\n",
    "document_lengths = [len(document) for document in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_lengths "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct_words = set(word for document in documents for word in document)\n",
    "W = len(distinct_words)\n",
    "\n",
    "D = len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_topic_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_topic_given_document(topic: int, d: int, alpha: float = 0.1) -> float:\n",
    "    \"\"\"\n",
    "    The fraction of words in document _d_\n",
    "    that are assigned to _topic_ (plus some smoothing)\n",
    "    \"\"\"\n",
    "    return ((document_topic_counts[d][topic] + alpha) /\n",
    "            (document_lengths[d] + K * alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_word_given_topic(word: str, topic: int, beta: float = 0.1) -> float:\n",
    "    \"\"\"\n",
    "    The fraction of words assigned to _topic_\n",
    "    that equal _word_ (plus some smoothing)\n",
    "    \"\"\"\n",
    "    return ((topic_word_counts[topic][word] + beta) /\n",
    "            (topic_counts[topic] + W * beta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_weight(d: int, word: str, k: int) -> float:\n",
    "    \"\"\"\n",
    "    Given a document and a word in that document,\n",
    "    return the weight for the kth topic\n",
    "    \"\"\"\n",
    "    return p_word_given_topic(word, k) * p_topic_given_document(k, d)\n",
    "\n",
    "def choose_new_topic(d: int, word: str) -> int:\n",
    "    return sample_from([topic_weight(d, word, k)\n",
    "                        for k in range(K)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "document_topics = [[random.randrange(K) for word in document]\n",
    "                   for document in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in range(D):\n",
    "    for word, topic in zip(documents[d], document_topics[d]):\n",
    "        document_topic_counts[d][topic] += 1\n",
    "        topic_word_counts[topic][word] += 1\n",
    "        topic_counts[topic] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_topic_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:01<00:00, 679.96it/s]\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "for iter in tqdm.trange(1000):\n",
    "    for d in range(D):\n",
    "        for i, (word, topic) in enumerate(zip(documents[d],\n",
    "                                              document_topics[d])):\n",
    "\n",
    "            # remove this word / topic from the counts\n",
    "            # so that it doesn't influence the weights\n",
    "            document_topic_counts[d][topic] -= 1\n",
    "            topic_word_counts[topic][word] -= 1\n",
    "            topic_counts[topic] -= 1\n",
    "            document_lengths[d] -= 1\n",
    "\n",
    "            # choose a new topic based on the weights\n",
    "            new_topic = choose_new_topic(d, word)\n",
    "            document_topics[d][i] = new_topic\n",
    "\n",
    "            # and now add it back to the counts\n",
    "            document_topic_counts[d][new_topic] += 1\n",
    "            topic_word_counts[new_topic][word] += 1\n",
    "            topic_counts[new_topic] += 1\n",
    "            document_lengths[d] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Java 3\n",
      "0 Big Data 3\n",
      "0 Hadoop 2\n",
      "0 HBase 1\n",
      "0 C++ 1\n",
      "0 Spark 1\n",
      "0 Storm 1\n",
      "0 programming languages 1\n",
      "0 MapReduce 1\n",
      "0 Cassandra 1\n",
      "0 deep learning 1\n",
      "1 HBase 2\n",
      "1 neural networks 2\n",
      "1 Postgres 2\n",
      "1 MongoDB 2\n",
      "1 machine learning 2\n",
      "1 Cassandra 1\n",
      "1 numpy 1\n",
      "1 decision trees 1\n",
      "1 deep learning 1\n",
      "1 databases 1\n",
      "1 MySQL 1\n",
      "1 NoSQL 1\n",
      "1 artificial intelligence 1\n",
      "1 scipy 1\n",
      "2 regression 3\n",
      "2 Python 2\n",
      "2 R 2\n",
      "2 libsvm 2\n",
      "2 scikit-learn 2\n",
      "2 mathematics 1\n",
      "2 support vector machines 1\n",
      "2 Haskell 1\n",
      "2 Mahout 1\n",
      "3 statistics 3\n",
      "3 probability 3\n",
      "3 Python 2\n",
      "3 R 2\n",
      "3 pandas 2\n",
      "3 statsmodels 2\n",
      "3 C++ 1\n",
      "3 artificial intelligence 1\n",
      "3 theory 1\n"
     ]
    }
   ],
   "source": [
    "for k, word_counts in enumerate(topic_word_counts):\n",
    "    for word, count in word_counts.most_common():\n",
    "        if count > 0:\n",
    "            print(k, word, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hadoop', 'Big Data', 'HBase', 'Java', 'Spark', 'Storm', 'Cassandra']\n",
      "Big Data and programming languages 7\n",
      "['NoSQL', 'MongoDB', 'Cassandra', 'HBase', 'Postgres']\n",
      "Python and statistics 5\n",
      "['Python', 'scikit-learn', 'scipy', 'numpy', 'statsmodels', 'pandas']\n",
      "Python and statistics 2\n",
      "databases 2\n",
      "machine learning 2\n",
      "['R', 'Python', 'statistics', 'regression', 'probability']\n",
      "machine learning 3\n",
      "databases 2\n",
      "['machine learning', 'regression', 'decision trees', 'libsvm']\n",
      "databases 2\n",
      "Python and statistics 2\n",
      "['Python', 'R', 'Java', 'C++', 'Haskell', 'programming languages']\n",
      "databases 3\n",
      "Big Data and programming languages 3\n",
      "['statistics', 'probability', 'mathematics', 'theory']\n",
      "machine learning 3\n",
      "databases 1\n",
      "['machine learning', 'scikit-learn', 'Mahout', 'neural networks']\n",
      "databases 2\n",
      "Python and statistics 2\n",
      "['neural networks', 'deep learning', 'Big Data', 'artificial intelligence']\n",
      "Python and statistics 3\n",
      "Big Data and programming languages 1\n",
      "['Hadoop', 'Java', 'MapReduce', 'Big Data']\n",
      "Big Data and programming languages 4\n",
      "['statistics', 'R', 'statsmodels']\n",
      "machine learning 3\n",
      "['C++', 'deep learning', 'artificial intelligence', 'probability']\n",
      "machine learning 3\n",
      "Big Data and programming languages 1\n",
      "['pandas', 'R', 'Python']\n",
      "machine learning 3\n",
      "['databases', 'HBase', 'Postgres', 'MySQL', 'MongoDB']\n",
      "Python and statistics 5\n",
      "['libsvm', 'regression', 'support vector machines']\n",
      "databases 3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "topic_names = [\"Big Data and programming languages\",\n",
    "               \"Python and statistics\",\n",
    "               \"databases\",\n",
    "               \"machine learning\"]\n",
    "\n",
    "for document, topic_counts in zip(documents, document_topic_counts):\n",
    "    print(document)\n",
    "    for topic, count in topic_counts.most_common():\n",
    "        if count > 0:\n",
    "            print(topic_names[topic], count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from linear_algebra import dot, Vector\n",
    "import math\n",
    "\n",
    "def cosine_similarity(v1: Vector, v2: Vector) -> float:\n",
    "    return dot(v1, v2) / math.sqrt(dot(v1, v1) * dot(v2, v2))\n",
    "\n",
    "assert cosine_similarity([1., 1, 1], [2., 2, 2]) == 1, \"same direction\"\n",
    "assert cosine_similarity([-1., -1], [2., 2]) == -1,    \"opposite direction\"\n",
    "assert cosine_similarity([1., 0], [0., 1]) == 0,       \"orthogonal\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\"red\", \"green\", \"blue\", \"yellow\", \"black\", \"\"]\n",
    "nouns = [\"bed\", \"car\", \"boat\", \"cat\"]\n",
    "verbs = [\"is\", \"was\", \"seems\"]\n",
    "adverbs = [\"very\", \"quite\", \"extremely\", \"\"]\n",
    "adjectives = [\"slow\", \"fast\", \"soft\", \"hard\"]\n",
    "\n",
    "def make_sentence() -> str:\n",
    "    return \" \".join([\n",
    "        \"The\",\n",
    "        random.choice(colors),\n",
    "        random.choice(nouns),\n",
    "        random.choice(verbs),\n",
    "        random.choice(adverbs),\n",
    "        random.choice(adjectives),\n",
    "        \".\"\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = make_sentence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The green bed is extremely slow .'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SENTENCES = 50\n",
    "\n",
    "random.seed(0)\n",
    "sentences = [make_sentence() for _ in range(NUM_SENTENCES)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
